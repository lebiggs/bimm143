---
title: "MiniProject"
author: "Laura Biggs"
format: pdf
---

## Import cancer data
```{r}
fna.data <- "WisconsinCancer.csv" 

#Store as a dataframe and convert sample names to row names
wisc.df <- read.csv(fna.data, row.names=1)
head(wisc.df)
dim(wisc.df)
```

Omit the diagnosis column from the data frame
```{r}
wisc.data <- wisc.df[,-1]

#Create diagnosis vector for later
diagnosis <- wisc.df[,1]
head(diagnosis)
```


## Exploratory data analysis

Q1. How many observations are in this dataset?
```{r}
# Number of rows and columns in wisc.data
dim(wisc.data)

# Number of observations in diagnosis
length(diagnosis)
```

Q2. How many of the observations have a malignant diagnosis?
```{r}
# Returns table of malignant and benign samples
table(diagnosis)
```

Q3. How many variables/features in the data are suffixed with _mean?
```{r}
grep("_mean", names(wisc.data))

grep("_mean", diagnosis)
```


## PCA

Check column means and SD; need to scale
```{r}
colMeans(wisc.data)
apply(wisc.data, 2, sd)

```

PCA on wisc.data
```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)
```

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
44% of the variance is accounted for by PC1. From summary(wisc.pr).

Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
At a minimum, the first 3 PCs are required to desribe 70% of the variance. From summary(wisc.pr).

Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
7 PCs are required to describe at least 90% of the original variance. From summary(wisc.pr).

Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
Not easy to understand as there are many data points crowding the plot. There is no discernable pattern to make senseof the PCs.
```{r}
biplot(wisc.pr)
```

Scatter plot of PC1 and PC2
```{r}
# Force R to recognize diagnosis as factor to plot
diagnosis = as.factor(diagnosis)
plot(wisc.pr$x[,1], wisc.pr$x[,2],
     xlab = "PC1", ylab = "PC2", col = diagnosis)
```

Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
Plots are very similar with the exception that PC3 shifts the points downwards and PC1 consistently accounts for the differene between malignant and benign samples.
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3],
     xlab = "PC1", ylab = "PC3", col = diagnosis)
```

ggplot of PCA
```{r}
#Create dataframe and add diagnosis column
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

#Load ggplot2
library(ggplot2)

#Scatter plot colored by diagnosis
ggplot(df) +
  aes(PC1, PC2, col = diagnosis) +
  geom_point()
```

Variance of each component
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Variance explained by each PC
```{r}
pve <- pr.var/sum(pr.var)
pve

#Plot of variance
plot(pve, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0,1), type = "o")
```

Alternative scree plot
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
The loading score for concave.points_mean from PC1 is -0.26.
```{r}
wisc.pr$rotation[,1]
barplot(wisc.pr$rotation[,1], las=2)
```


Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
5 PCs are the minimum number of principal components needed to explain 80% of the variance.
```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)

var <- round((wisc.pr$sdev^2)/sum(wisc.pr$sdev^2) * 100)
var
```

## Hierarchical clustering

Scale the wisc.data
```{r}
data.scaled <- scale(wisc.data)

#Eucladian distance
data.dist <- dist(data.scaled)

#Hierachical clustering assignment
wisc.hclust <- hclust(data.dist, method = "complete")
```

Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h = 19, col="red", lty=2)
```

Number of clusters
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)
```

Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
Cluster 4 is the best cluster as additional clusters do not add greater separation.
```{r}
# k = 2
#wisc.hclust.clusters <- cutree(wisc.hclust, k=2)
#table(wisc.hclust.clusters, diagnosis)

# k =3
#wisc.hclust.clusters <- cutree(wisc.hclust, k=3)
#table(wisc.hclust.clusters, diagnosis)

# k = 5
#wisc.hclust.clusters <- cutree(wisc.hclust, k=5)
#table(wisc.hclust.clusters, diagnosis)

# k = 6
#wisc.hclust.clusters <- cutree(wisc.hclust, k=6)
#table(wisc.hclust.clusters, diagnosis)

# k = 7
#wisc.hclust.clusters <- cutree(wisc.hclust, k=7)
#table(wisc.hclust.clusters, diagnosis)

# k = 8
#wisc.hclust.clusters <- cutree(wisc.hclust, k=8)
#table(wisc.hclust.clusters, diagnosis)

# k = 9
#wisc.hclust.clusters <- cutree(wisc.hclust, k=9)
#table(wisc.hclust.clusters, diagnosis)

# k = 10
#wisc.hclust.clusters <- cutree(wisc.hclust, k=10)
#table(wisc.hclust.clusters, diagnosis)
```

Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
Ward.D2 is my favorite as it is the most aesthetically pleasing and easiest to interpret.
```{r}
wisc.hclust_complete <- hclust(data.dist, method = "complete")
plot(wisc.hclust_complete)

wisc.hclust_single <- hclust(data.dist, method = "single")
plot(wisc.hclust_single)

wisc.hclust_average <- hclust(data.dist, method = "average")
plot(wisc.hclust_average)

wisc.hclust_ward.D2 <- hclust(data.dist, method = "ward.D2")
plot(wisc.hclust_ward.D2)
```

## K means clustering
```{r}
wisc.km <- kmeans(scale(wisc.data), centers= 2, nstart= 20)
table(wisc.km$cluster, diagnosis)
```




Q14. Optional

## Combining results
```{r}
wisc.pr.hclust <- hclust(data.dist, method = "ward.D2")

grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
table(grps, diagnosis)

# Plot by group
plot(wisc.pr$x[,1:2], col=grps)
plot(wisc.pr$x[,1:2], col=diagnosis)
```

First 7 PCs
```{r}
# Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")

# 2 clusters
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

Q15. How well does the newly created model with four clusters separate out the two diagnoses?
This model separates the four clusters well and minimizes the clusters into 2.
```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```

Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.
The k means model approximates this new model much more closely while maintaining clear clustering. The hierarchical clustering model isn't the best approximation of this new model as it contains 2 additional clusters.
```{r}
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)

```

## Sensitivity/Specificity

Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?
The k means method provides the highest sensitivity, while the hierarchical clustering method has the highest specificity.
```{r}
#Combined method sensitivity
188/(188+24)

#Combined method specificity
329/(329+28)

#K means method sensitivity
175/(175+14)

#K means method specificity
343/(343+37)

#hc method sensitivity
165/(165+5+40+2)

#hc method specificity
343/(12+2+343)
```

## Prediction

Load in new sample data
```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

Plot predicted data
```{r}
plot(wisc.pr$x[,1:2], col = diagnosis)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

Q18. Which of these new patients should we prioritize for follow up based on your results?
Patient 2 is localized to PC2 which comprises the majority of the malignant samples. 